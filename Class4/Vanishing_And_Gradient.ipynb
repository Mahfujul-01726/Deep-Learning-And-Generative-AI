{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# Simulating Vanishing & Exploding Gradients in a Deep Network\n",
        "class DeepNetwork(tf.keras.Model):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation='sigmoid', init='normal'):\n",
        "        super(DeepNetwork, self).__init__()\n",
        "        self.layers_list = []\n",
        "        self.activation = activation\n",
        "\n",
        "        # Add multiple hidden layers\n",
        "        for _ in range(50):  # 50-layer deep network\n",
        "            layer = layers.Dense(hidden_dim)\n",
        "            if init == 'normal':\n",
        "                layer.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=2.0)  # Large weights (causes exploding gradients)\n",
        "            elif init == 'xavier':\n",
        "                layer.kernel_initializer = tf.keras.initializers.GlorotUniform()  # Xavier Initialization (prevents vanishing)\n",
        "            self.layers_list.append(layer)\n",
        "\n",
        "        self.output_layer = layers.Dense(output_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        for layer in self.layers_list:\n",
        "            x = layer(x)\n",
        "            if self.activation == 'sigmoid':\n",
        "                x = tf.nn.sigmoid(x)  # Causes vanishing gradients\n",
        "            elif self.activation == 'relu':\n",
        "                x = tf.nn.relu(x)  # Helps mitigate vanishing\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Input, Hidden, and Output dimensions\n",
        "input_dim = 10\n",
        "hidden_dim = 100\n",
        "output_dim = 1\n",
        "\n",
        "# Creating models for vanishing and exploding gradients\n",
        "model_vanishing = DeepNetwork(input_dim, hidden_dim, output_dim, activation='sigmoid', init='normal')\n",
        "model_exploding = DeepNetwork(input_dim, hidden_dim, output_dim, activation='relu', init='normal')\n",
        "\n",
        "# Loss function and Optimizer\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "optimizer_vanishing = keras.optimizers.SGD(learning_rate=0.01)\n",
        "optimizer_exploding = keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "# Generate random input data\n",
        "x = np.random.randn(32, input_dim).astype(np.float32)\n",
        "y = np.random.randn(32, output_dim).astype(np.float32)\n",
        "\n",
        "# Training and Observing Gradients\n",
        "def train_model(model, optimizer, name):\n",
        "    with tf.GradientTape() as tape:\n",
        "        output = model(x)\n",
        "        loss = loss_fn(y, output)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Print gradient norms\n",
        "    total_norm = sum(tf.norm(g).numpy() for g in grads if g is not None)\n",
        "    print(f\"{name} - Gradient Norm: {total_norm:.4f}\")\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "# Train and observe gradient issues\n",
        "train_model(model_vanishing, optimizer_vanishing, \"Vanishing Gradients Model\")\n",
        "train_model(model_exploding, optimizer_exploding, \"Exploding Gradients Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EDUGslbcgt4",
        "outputId": "b66c7ed0-ac17-4413-a418-50904d977a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanishing Gradients Model - Gradient Norm: 3404836.7383\n",
            "Exploding Gradients Model - Gradient Norm: nan\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simulating Vanishing & Exploding Gradients in a Deep Network\n",
        "class DeepNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, activation='sigmoid', init='normal'):\n",
        "        super(DeepNetwork, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.activation = activation\n",
        "\n",
        "        # Add the first hidden layer with the correct input dimension\n",
        "        layer = nn.Linear(input_dim, hidden_dim)  # Changed from hidden_dim to input_dim\n",
        "        self.layers.append(layer)\n",
        "        if init == 'normal':\n",
        "            nn.init.normal_(layer.weight, mean=0, std=2.0)\n",
        "        elif init == 'xavier':\n",
        "            nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "        # Add multiple hidden layers (keep the rest unchanged)\n",
        "        for _ in range(49):  # 49 layers now to keep a total of 50\n",
        "            layer = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.layers.append(layer)\n",
        "            if init == 'normal':\n",
        "                nn.init.normal_(layer.weight, mean=0, std=2.0)\n",
        "            elif init == 'xavier':\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            if self.activation == 'sigmoid':\n",
        "                x = torch.sigmoid(x)\n",
        "            elif self.activation == 'relu':\n",
        "                x = torch.relu(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Input, Hidden, and Output dimensions\n",
        "input_dim = 10\n",
        "hidden_dim = 100\n",
        "output_dim = 1\n",
        "\n",
        "# Creating models for vanishing and exploding gradients\n",
        "model_vanishing = DeepNetwork(input_dim, hidden_dim, output_dim, activation='sigmoid', init='normal')\n",
        "model_exploding = DeepNetwork(input_dim, hidden_dim, output_dim, activation='relu', init='normal')\n",
        "\n",
        "# Loss function and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_vanishing = optim.SGD(model_vanishing.parameters(), lr=0.01)\n",
        "optimizer_exploding = optim.SGD(model_exploding.parameters(), lr=0.01)\n",
        "\n",
        "# Generate random input data\n",
        "x = torch.randn(32, input_dim)\n",
        "y = torch.randn(32, output_dim)\n",
        "\n",
        "# Training and Observing Gradients\n",
        "def train_model(model, optimizer, name):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Print gradient norms\n",
        "    total_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)\n",
        "    print(f\"{name} - Gradient Norm: {total_norm:.4f}\")\n",
        "    optimizer.step()\n",
        "\n",
        "# Train and observe gradient issues\n",
        "train_model(model_vanishing, optimizer_vanishing, \"Vanishing Gradients Model\")\n",
        "train_model(model_exploding, optimizer_exploding, \"Exploding Gradients Model\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Rkhib3G8c3Lh",
        "outputId": "0070653f-36be-48b8-d67a-2f592200a302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanishing Gradients Model - Gradient Norm: 507532.2496\n",
            "Exploding Gradients Model - Gradient Norm: nan\n"
          ]
        }
      ]
    }
  ]
}